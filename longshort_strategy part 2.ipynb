{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330f5735",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"longshort_strategy part 2.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efbb5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7db535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isclose(value, original, tolerance= 0.05):\n",
    "    return value <= original * (1+tolerance) and value >= original * (1-tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5390a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this homework we will\n",
    "# introduce pandas through data cleaning and processing\n",
    "# implement a basic trading strategy\n",
    "# model the costs of a trading strategy\n",
    "# calculate expected returns\n",
    "# use expected returns to create a trading strategy\n",
    "# try to add predictive features/signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1e415",
   "metadata": {},
   "source": [
    "# returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90051a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to know how any strategy would perform, we need to be able to calculate the return on investment\n",
    "# in finance, we frequently use log returns rather than simple returns\n",
    "# this is because log returns are easier to work with\n",
    "# log returns are treated as constantly compounding return, so we can add them over time, rather than multiplying\n",
    "# we can calculate the simple returns as (x1-x0)/x0, the percentage change\n",
    "# log returns are calculated as log(x1/x0)\n",
    "# see if you can derive the transformations from log returns to simple returns, and simple returns to log returns\n",
    "hdf = pd.read_parquet('stock_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c6454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use np.exp\n",
    "def logtosimple(logreturn):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dafea5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdab521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use np.log\n",
    "def simpletolog(simplereturn):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8dbd3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53958da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's look at an example asset\n",
    "# create a dataframe that is just the adjusted close and volume for AAL\n",
    "aal = hdf.loc[:,hdf.columns.get_level_values(1) == ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee424941",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0588dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's remove the top level of the columns in order to make things easier - we already know the symbol is AAL\n",
    "aal.columns = aal.columns.get_level_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6337c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef10ba03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can create a new column using\n",
    "# df['newcolname'] = series\n",
    "# let's create a new column for the returns in AAL\n",
    "# we can get the returns using .pct_change() on the adjusted close column\n",
    "aal['returns'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5ec35",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb02d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can get the log returns by adjusting the percent change function\n",
    "\n",
    "aal['logreturns'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502c274",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb5c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# .cumsum() on a series will add everything in the column up until that point\n",
    "# this will give us the log return from the start, to the current index\n",
    "aal['cum_logreturns'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5ecb0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas also comes equipped with some built in plotting\n",
    "# we can plot the adjusted close, as well as the logreturns cumsum, and see if they look the same (they should)\n",
    "aal['Adj Close'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef464fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "aal['cum_logreturns'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1dde33",
   "metadata": {},
   "source": [
    "# basic trend strategy: long/short x%-ile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d734273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to try to create a trading strategy\n",
    "# we'll assume that we can short, and assume that the cost to borrow is 0 (not trivial assumption)\n",
    "# our trading strategy is going to try to buy stocks that will go up, and sell stocks that will go down\n",
    "# one way to do this is to try to rank the stocks based on some metric\n",
    "# and go long/go short the top/bottom percent by that metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d734274",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to more easily group by asset, we'll make assets into its own column using df.stack()\n",
    "hdf_old = hdf.copy()\n",
    "hdf = (\n",
    "    hdf.stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={'level_1': 'Symbol'})\n",
    ")\n",
    "hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a560a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we calculate returns and log returns for the entire dataframe\n",
    "# we can **group by the ticker**, select the adjusted close column, and then use .pct_change() to get the percentage change from one row to the next\n",
    "hdf['returns'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd30be6b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe2657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we'll also create a log returns column\n",
    "# we do this by creating the logreturns column using our simple to log function\n",
    "hdf['logreturns'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dc6e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51977b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we want to predict the next step of returns, so we need to create a column for that \n",
    "# calculate the forward logreturn by **grouping by the ticker**, selecting the logreturns column,\n",
    "# and then shifting the series up a row with .shift(-1)\n",
    "\n",
    "hdf['fwd_logreturn'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8424c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c6cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's also drop all of the rows where logreturns and forward logreturns have NaN values, as these will mess up future operations\n",
    "# we can do this with the subset parameter in df.dropna(subset=[])\n",
    "hdf = hdf.dropna(subset=[...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006e1a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083b60a",
   "metadata": {},
   "source": [
    "# momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can rank the returns by each asset\n",
    "# what metric should we use? \n",
    "\n",
    "# you might notice that stocks that went up tend to continue to go up, \n",
    "# and stocks that go down tend to continue to go down\n",
    "# this economically makes sense because people who see that a stock went up by a lot might buy it to join the trend, and vice versa\n",
    "\n",
    "# so we can figure out which assets went up the most, and buy those\n",
    "# and we can short the assets that went down the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92688dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can calculate this using the .rank() method\n",
    "# it will output the rank value, if the dataframe was sorted \n",
    "# we'll use 'ascending=False', in order to make the\n",
    "# assign the logreturn_rank column to a rank of the logreturns column, grouped by date\n",
    "# be sure to use .rank(method='dense')\n",
    "\n",
    "hdf['logreturn_rank'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e75e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84049bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the mean log returns for each date\n",
    "date_mean_logreturns = hdf.groupby('Date')['logreturns'].transform('mean')\n",
    "\n",
    "# Calculate the demeaned log returns\n",
    "hdf['demeaned_logreturn'] = hdf['logreturns'] - date_mean_logreturns\n",
    "\n",
    "# Calculate the mean forward log returns for each date\n",
    "date_mean_fwd_logreturns = hdf.groupby('Date')['fwd_logreturn'].transform('mean')\n",
    "\n",
    "# Calculate the demeaned forward log returns\n",
    "hdf['demeaned_fwd_logreturn'] = hdf['fwd_logreturn'] - date_mean_fwd_logreturns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f44e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can also look at the demeaned returns, or the returns of the asset we chose relative to all assets\n",
    "# this will help us assess if we were able to choose the assets that would end up under/overperforming\n",
    "\n",
    "date_mean_logreturns = hdf.groupby('Date')['logreturns'].transform('mean')\n",
    "\n",
    "# Calculate the demeaned log returns\n",
    "hdf['demeaned_logreturn'] = hdf['logreturns'] - date_mean_logreturns\n",
    "\n",
    "# Calculate the mean forward log returns for each date\n",
    "date_mean_fwd_logreturns = ...\n",
    "\n",
    "# Calculate the demeaned forward log returns\n",
    "hdf['demeaned_fwd_logreturn'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f196d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ease of analyzing the effect, we will bucket our feature using a decile (each bin is 10%-ile)\n",
    "# this will help us figure out how strong the effect is\n",
    "# we do this using pd.qcut(x, q=quantile, labels=False, duplicates='drop'), which cuts a series into quantile buckets. pass q=10 to get decile buckets\n",
    "# drop labels and duplicates\n",
    "hdf['logreturn_decile'] = hdf.groupby('Date')['logreturns'].transform(\n",
    "    lambda x: pd.qcut(x, q=10, labels=False, duplicates='drop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98354b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll plot this using a barplot\n",
    "hdf.groupby('logreturn_decile').mean(numeric_only = True)['demeaned_fwd_logreturn'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b498e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it definitely looks like the bottom and top 20%-ile have outsized returns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba518d",
   "metadata": {},
   "source": [
    "# strategy 'backtesting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while backtesting can be a useful tool, its often a bit overkill when making an initial test to see if a feature is useful\n",
    "# here we will do a very basic backtest: assuming no trading costs\n",
    "# costs to trade are a huge drag on our edge, but not worth thinking about at least for now\n",
    "# the idea is that if it doesnt work when there are no costs, it probably won't work when there are costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62af09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's define our strategy by setting an asset to be long or short if its in the top or bottom 20% log return decile\n",
    "# we'll also equal weight all our positions\n",
    "# the decile values for the bottom 20% would be 0 and 1\n",
    "# the decile values for the top 20% would be 8 and 9\n",
    "\n",
    "# we can use np.where() in order to conditionally assign a column\n",
    "hdf['long/short'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d8548b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6da4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we size our positions by taking our absolute long/short position\n",
    "hdf['absposition'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012de31",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925fb25b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting the total number of our positions by summing our absolute position for each day \n",
    "hdf['numpositions'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f8bde",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f452fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and we get our final weight for each asset by scaling our long/short indicator variable by the number of positions we have\n",
    "# each position should be such that we add up to one, so we'd divide the indicator by total positions\n",
    "# if we have 0 positions, the weight should be 0\n",
    "# this will mean that we should have equal size long and short, adding up to a total of 1 (no leverage)\n",
    "# we'll use df.apply(lambda row: (expression using row) if (condition on some row) else value, axis=1)\n",
    "\n",
    "hdf['weight'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6c4a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090a27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# finally we define the strategy's return as the weighted logreturns based on our position\n",
    "# so we multiply weight by forward logreturn\n",
    "hdf['strategy_logreturn'] = .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0054f4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4332f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can compare this with an equal weighted strategy, where we long each asset the same amount, again summing up to 1\n",
    "hdf['eqweight'] = ...\n",
    "hdf['equal_logreturn'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0e674",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690bd67",
   "metadata": {},
   "source": [
    "# evaluating performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ce1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.groupby('Date').sum()['strategy_logreturn'].cumsum().plot(legend=True)\n",
    "hdf.groupby('Date').sum()['equal_logreturn'].cumsum().plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c549ef3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can get the final return by getting the last value of the column (values[-1])\n",
    "# and we can translate that to simple returns for readability\n",
    "final_strategyreturn = logtosimple(hdf.groupby('Date').sum()['strategy_logreturn'].cumsum().values[-1])\n",
    "final_equalreturn = ...\n",
    "\n",
    "print(f'final_strategyreturn: {final_strategyreturn}')\n",
    "print(f'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a60ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the equal strategy way outperformed our strategy! since our strategy was rather simple, this isn't too unexpected\n",
    "# if you notice, the equal strategy was rather volatility\n",
    "\n",
    "# one way to account for this is using a sharpe ratio\n",
    "# a sharpe ratio is essentially a t-test for the statistical significance of a strategy's risk adjusted return\n",
    "# its the mean return of the strategy minus the risk free rate, divided by the standard deviation of return of the strategy\n",
    "\n",
    "# generally, any sharpe ratio over 1 is good, 2 is very good, 3+ is very very good\n",
    "# as you see a sharpe ratio > 3, the more likely that the strategy is somehow limited, or you've calculated something wrong\n",
    "# there are many simplifications in this code, so don't worry about the sharpe ratios, they are not realistic\n",
    "# as an example: limiting our universe to companies that are currently in the SP500 introduces a lookahead bias\n",
    "# we are using information (stocks still listed in the sp500 as of today) that we wouldnt have had at the time (back in 2023)\n",
    "# try to think of more ways this sample or testing strategy might be limited!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a37d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's calculate the sharpe ratio with a function, we'll leave out the risk free rate part of it for now\n",
    "# we also need to normalize the sharpe ratio with respect to a year, by multiplying by the square root of periods our strategy trades in a year\n",
    "# note that there are 252 trading days\n",
    "\n",
    "def sharpe_ratio(mean_ret, std_ret):\n",
    "    return ... * np.sqrt(...)\n",
    "    \n",
    "strat_sharpe = sharpe_ratio(hdf.groupby('Date').sum()['strategy_logreturn'].mean(), \n",
    "             hdf.groupby('Date').sum()['strategy_logreturn'].std())\n",
    "equal_sharpe = sharpe_ratio(..., \n",
    "             ...)\n",
    "print(strat_sharpe, equal_sharpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dee441",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f306fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting: it looks like the equal strategy had a very high sharpe\n",
    "# this is because 2023 oct to 2024 march was a very bullish time (you can look up the S&P 500 returns back then to check)\n",
    "# usually, we would want to backtest over a longer time period, such as over a year, to reduce variance from such market idiosyncrasies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c998c9b",
   "metadata": {},
   "source": [
    "# accounting for fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec900333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is without fees so it is clearly way too good\n",
    "# let's add a fee for each trade, and expected slippage per trade\n",
    "# the fee is what we would pay to the broker, and the expected slippage is likely a function of our position size\n",
    "# we'll combine these into one value, and just observe how our strategy decays as a function of cost\n",
    "\n",
    "# let's define a percentage fee per trade (e.g., 0.02%)\n",
    "fee = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a493af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in order to see how large our trade would be, we have to find the difference between our previous and current position size\n",
    "# we should sort values by symbol, then by the date\n",
    "hdf = hdf.sort_values(by=[...]) # order matters!\n",
    "\n",
    "# we'll groupby symbol, and then get the previous weight by using .shift(1) to shift the weights down\n",
    "hdf['prevweight'] = hdf.groupby('Symbol')['weight'] ...\n",
    "\n",
    "# next, we'll get the strategy's weight change by taking the difference between weight and prevweight\n",
    "hdf['strategy_weightchange'] = ...\n",
    "\n",
    "# finally, to calculate fees, we'll need to multiply the fee by our absolute change in position\n",
    "hdf['strategy_fees'] = abs(hdf['strategy_weightchange']) * fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3508b4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c976ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we'll calculate the strategy log return after fees by subtracting the groupby fees from the groupby logreturn\n",
    "hdf['strategy_postfees'] = ...\n",
    "strategy_postfees_seriestoplot = ...\n",
    "strategy_postfees_seriestoplot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e314861",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8162cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and we'll test the sharpe and logreturn as above\n",
    "fees_sharpe = sharpe_ratio(..., \n",
    "                           ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969901c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the equal weight buy and hold does not change with fees, as it never changes position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c952ef",
   "metadata": {},
   "source": [
    "# expected returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a0c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have a very basic long/short strategy, we should try to improve upon it\n",
    "\n",
    "# our strategy roughly was equally long and short the market - regardless of how strongly something moved\n",
    "# even if all of the longs were very high return, the strategy didn't care\n",
    "# it also didn't care if it was 10th or 20th decile: we gave the same weight regardless\n",
    "# we might expect that there's a way to improve upon this\n",
    "\n",
    "# one way of doing this is trying to calculate an 'expected return' for each asset\n",
    "# this allows us to weight our positions based on how good we think they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b956b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how might we make an expected returns model? \n",
    "# we'll likely want to fit to some historical data, and see how that strategy performs on data after that\n",
    "\n",
    "# to avoid our model just learning the optimal answer for our entire dataset\n",
    "# we'll train the model on the first 80% of our data\n",
    "# and see how it performs on the remaining 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb9fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the data into training and testing using .iloc\n",
    "train_percent = ... # use 80% as a decimal\n",
    "# make sure to split according to time series!\n",
    "\n",
    "hdf = hdf.sort_values(...) # order matters! we need to split by time first, then asset\n",
    "\n",
    "# we can only use integer indices, so make sure to cast the value to an integer\n",
    "splitrow = ...\n",
    "training = hdf.iloc[:splitrow]\n",
    "testing = hdf.iloc[splitrow:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b5106",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very common basic model is a linear regression: fitting a line to points of data\n",
    "# given some x variable, we try to solve for the optimal y = mx + b, \n",
    "# we do this by minimizing the squared sum of differences of our line to each data point\n",
    "# luckily there are libraries that do this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d7aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run a linear regression on our training data, we need a data matrix X of features\n",
    "# and a target y to fit to \n",
    "# in our case, our target is forward log return\n",
    "# and our data matrix X is the current log returns\n",
    "# let's run the regression using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37b23b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature = [...]\n",
    "target = [...]\n",
    "\n",
    "# we add a constant to data matrix Xin order to get an intercept term, otherwise we would be fitting y = mx\n",
    "X = training[...]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = training[...]\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece38c4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d40d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we can plot our linear regression\n",
    "\n",
    "m = model.params[...]\n",
    "b = model.params[...]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(training[feature], y, color='blue', label='Data')\n",
    "\n",
    "# Plot the linear regression line\n",
    "plt.plot(training[feature], m * training[feature] + b, color='red', label='linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef023de",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a119d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we can see how well our model does on our testing data\n",
    "X_test = sm.add_constant(testing[feature])\n",
    "y_pred = model.predict(...).values\n",
    "\n",
    "# we compare the predictions to the actual values\n",
    "y_test = testing[...].values\n",
    "\n",
    "# we use mean squared error: the mean difference between y_test and y_pred, squared\n",
    "mse = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f0bd24",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad45e215",
   "metadata": {},
   "source": [
    "# updating backtest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74254ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also can look at our model's sharpe ratio on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ada5da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# we can get the expected log returns by applying our model, with a constant, to the feature column\n",
    "\n",
    "testing['ex_logreturns'] = model.predict(sm.add_constant(...))\n",
    "\n",
    "# we'll size our weights according to the cross sectional predictions\n",
    "# use transform again to do this\n",
    "# we want to have each weight be (ex_logreturn-mean ex_logreturn for date)/(sum of absolute ex_logreturns for date)\n",
    "\n",
    "testing['ex_weight'] = testing.groupby('Date')['ex_logreturns'].transform(lambda x: ...)\n",
    "\n",
    "# and we multiply the forward log returns again\n",
    "testing['exstrategy_logreturns'] = ...\n",
    "\n",
    "# and we plot once more\n",
    "testing.groupby('Date').sum()['exstrategy_logreturns'].cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b703d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q10a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14bbff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wow that looks very promising! let's calculate the sharpe ratio with fees\n",
    "# sort again\n",
    "testing = testing.sort_values(...)\n",
    "# get previous ex_weight as before\n",
    "testing['prevex_weight'] = ...\n",
    "# get change in weight as before\n",
    "testing['exstrategy_weightchange'] = ...\n",
    "\n",
    "# get absolute change in position\n",
    "testing['exstrategy_fees'] = ...\n",
    "# and find the return post fees\n",
    "testing['exstrategy_postfees'] = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc54ea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q10b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db56cb0",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to add more features to see if we can make this model any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c453c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# first, let's create a function that makes the bar plot from before, so we can easily view any feature\n",
    "def summarize_feature(hdf, colname):\n",
    "    if hdf[colname].dtype == 'float':\n",
    "        # use qcut here, by deciles as before\n",
    "        bins = ...\n",
    "        hdf.groupby(bins).mean()['demeaned_fwd_logreturn'].plot(kind='bar')\n",
    "    else:\n",
    "        hdf.groupby(colname).mean()['demeaned_fwd_logreturn'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3406af96",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q11a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f1e99e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's reincorporate sector data into this dataframe\n",
    "# we can do this using the merge function in pandas\n",
    "columns\n",
    "hdf = hdf.merge(df[[columns]], on='Symbol', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3bf87",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q11b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9da90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# another possible feature idea could be notional volume, or the dollar amount of shares traded\n",
    "# we can get notional volume by multiplying the close price with the shares traded\n",
    "hdf['ntlvolume'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd7188",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q11c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to create more features here, and use the below functions to test them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24cb39",
   "metadata": {},
   "source": [
    "# testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914afb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(hdf, features, train_test_split=0.8, debug=0):\n",
    "    # split the data into training and testing using .iloc\n",
    "    train_percent = 0.8\n",
    "    \n",
    "    # make sure to split according to time series!\n",
    "    hdf = hdf.sort_values(['Date', 'Symbol'])\n",
    "    training = hdf.iloc[:int(len(hdf) * train_percent)]\n",
    "    testing = hdf.iloc[int(len(hdf) * train_percent):]\n",
    "    \n",
    "    target = ['fwd_logreturn']\n",
    "    \n",
    "    # Identify categorical features based on data type\n",
    "    categorical_features = training[features].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # one hot encoding for categorical features\n",
    "    # we essentially create a bunch of extra columns, and assign them as ones and zeros\n",
    "    training = pd.get_dummies(training, columns=categorical_features)\n",
    "    testing = pd.get_dummies(testing, columns=categorical_features)\n",
    "    \n",
    "    # Update the features list to include dummy variables\n",
    "    features = [col for col in training.columns if col in features or col.startswith(tuple(categorical_features))]\n",
    "    if debug > 0:\n",
    "        print(f'features: {features}')\n",
    "    X_train = training[features]\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    y_train = training[target]\n",
    "    \n",
    "    model = sm.OLS(y_train, X_train).fit()\n",
    "    if debug > 0:\n",
    "        print(f'r_squared: {model.rsquared}')\n",
    "    \n",
    "    # For testing data\n",
    "    X_test = testing[features]  # Include the same features used for training\n",
    "    X_test = sm.add_constant(X_test)\n",
    "    \n",
    "    return testing, model, X_test, debug  # Return testing data along with the model and testing features\n",
    "\n",
    "def basic_backtest(testing, model, X_test, debug, fee=0.0002):\n",
    "    testing['ex_logreturns'] = model.predict(X_test)  # Use X_test for prediction\n",
    "    testing['ex_weight'] = testing.groupby('Date')['ex_logreturns'].transform(lambda x: (x - x.mean()) / x.abs().sum())\n",
    "    testing['exstrategy_logreturns'] = testing['ex_weight'] * testing['fwd_logreturn']\n",
    "    testing = testing.sort_values(by=['Symbol', 'Date'])\n",
    "    testing['prevex_weight'] = testing.groupby('Symbol')['ex_weight'].shift(1)\n",
    "    testing['exstrategy_weightchange'] = testing['ex_weight'] - testing['prevex_weight']\n",
    "    testing['exstrategy_fees'] = abs(testing['exstrategy_weightchange']) * fee\n",
    "    testing['exstrategy_postfees'] = testing['strategy_logreturn'] - testing['exstrategy_fees']\n",
    "    if debug > 0:\n",
    "        testing.groupby('Date').sum()['exstrategy_fees']\n",
    "    testing.groupby('Date').sum()['exstrategy_postfees'].cumsum().plot()\n",
    "    print(f\"sharpe_ratio: {sharpe_ratio(testing.groupby('Date').sum()['exstrategy_postfees'].mean(), testing.groupby('Date').sum()['exstrategy_postfees'].std())}\")\n",
    "    if debug > 1:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for symbol in testing['Symbol'].unique():\n",
    "#             print(symbol)\n",
    "            asset_weights = testing[testing['Symbol'] == symbol].set_index('Date')['ex_weight']\n",
    "            asset_weights.plot(label=symbol)\n",
    "        plt.title('Rolling Weights of Assets')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Weight')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5772688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the debug levels to get more or less information\n",
    "# pass your features into the list here\n",
    "# be very careful when changing the above functions!\n",
    "features = ['logreturns']\n",
    "basic_backtest(*(load_model(hdf, features, debug=1)), fee=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's much more to cover in the realm of researching and testing systematic trading strategies\n",
    "# importantly, we haven't covered much about risk modeling, and portfolio optimization \n",
    "# check out https://www.alacra.com/alacra/help/barra_handbook_US.pdf \n",
    "# for a good introduction to risk modeling, if you're curious!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be3a38",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Please also check gradescope for any written assignments for this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f07a31",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cce917",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q10a": {
     "name": "q10a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(testing['exstrategy_logreturns'].mean(), 1.0620373849221123e-06)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q10b": {
     "name": "q10b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs(testing['exstrategy_postfees'].mean()), 9.185534624020494e-07)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q11a": {
     "name": "q11a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf.groupby('logreturns').mean()['demeaned_fwd_logreturn'].max(), 0.29863292329834545)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q11b": {
     "name": "q11b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf.shape == (56885, 24)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q11c": {
     "name": "q11c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['ntlvolume'].mean(),570866467.8644993)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(df_head5) == 5\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(columns) == 3\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert unique_sectors == 11\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert it_sectors == 65\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> it_sectors\n65",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1e": {
     "name": "q1e",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert specified_slice.shape == (5,6)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1f": {
     "name": "q1f",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert false_healthcare == 439\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1g": {
     "name": "q1g",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert energy['CIK'].sum() == 19373487\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1h": {
     "name": "q1h",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert first4cols_industrials.shape == (79,4)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1i": {
     "name": "q1i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert old_3_security == 'Bath & Body Works, Inc.'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1j": {
     "name": "q1j",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(mean_realestate_cik, 956436.7741935484)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf.shape == hdf.shape\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf.columns.size == 1006\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert broken_df.shape[1] == 8\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2d": {
     "name": "q2d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf.shape == (115, 998)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(logtosimple(1),1.718281828459045)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(simpletolog(1),0.6931471805599453)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert aal.shape == (115,2)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3d": {
     "name": "q3d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert list(aal.columns.get_level_values(0).values) == ['Adj Close', 'Volume']\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3e": {
     "name": "q3e",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(aal['returns'].mean(),0.0011150589519497269)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3f": {
     "name": "q3f",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(aal['logreturns'].mean(),0.0007902055534179764)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3g": {
     "name": "q3g",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(aal['cum_logreturns'].mean(),0.05021465552608881)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['returns'].mean(), 0.0017457801190514555)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['logreturns'].mean(), 0.0015072870289151654)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs((hdf['logreturns'] - hdf['fwd_logreturn']).mean()),7.94025278927628e-05)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4d": {
     "name": "q4d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf.shape == (56161, 7)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf['logreturn_rank'].values[0] == 392.0\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5b": {
     "name": "q5b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs(hdf['demeaned_fwd_logreturn'].mean()),\n...                np.abs((hdf['fwd_logreturn'] - date_mean_fwd_logreturns).mean()))\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6a": {
     "name": "q6a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs(hdf['long/short'].sum()), 1)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6b": {
     "name": "q6b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['absposition'].sum(),23116)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6c": {
     "name": "q6c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['numpositions'].sum(),11580916)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6d": {
     "name": "q6d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs(hdf['weight'].mean()), 8.745933687456209e-08)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6e": {
     "name": "q6e",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs(hdf['strategy_logreturn'].mean()), 8.559054707207952e-07)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6f": {
     "name": "q6f",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['equal_logreturn'].mean(), 3.1830282614279954e-06)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7a": {
     "name": "q7a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(final_equalreturn, 0.19573618511585034)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7c": {
     "name": "q7c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(equal_sharpe, 3.1044873685151626)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8a": {
     "name": "q8a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(equal_sharpe, 3.0157625925259532)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8b": {
     "name": "q8b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(equal_sharpe, 3.0157625925259532)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8c": {
     "name": "q8c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(equal_sharpe, 3.0436630129744557)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9a": {
     "name": "q9a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert training.shape == (45508, 22)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9b": {
     "name": "q9b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> model.rsquared\n0.05403784673350487",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert isclose(model.rsquared, 0.05333756613777707)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9c": {
     "name": "q9c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(model.params['logreturns'], 0.2324590858601042)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert isclose(model.params['const'], 0.0011427008449693891)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9d": {
     "name": "q9d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(mse, 0.0002948418585430239)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
